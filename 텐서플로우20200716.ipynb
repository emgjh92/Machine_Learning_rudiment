{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"텐서플로우20200716.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"S-v3HWtvzJj2","colab_type":"code","colab":{}},"source":["# 새로 colab실행할때마다 설치해야함\n","!pip install tensorflow==1.15.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MyDVsgu0a2Yd","colab_type":"code","colab":{}},"source":["#2020.07.15\n","\n","# Lab 2 Linear Regression - 선형 회귀\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility(재현성)-> 해도 되고 안해도 되는 코드\n","\n","# X and Y data -> training set\n","x_train = [1, 2, 3, 4, 5, 6]\n","y_train = [6, 7, 8, 9, 10, 11]\n","\n","# Try to find values for W and b to compute y_data = x_data * W + b\n","# We know that W should be 1 and b should be 0\n","# But let TensorFlow figure it out\n","# weight는 10, bias는 0으로 시작하겠다. \n","W = tf.Variable(10.0) # W = 10으로 하지 않음. Variable클래스를 통해 자동으로 변경해야할 변수를 만듦\n","b = tf.Variable(10.0)\n","\n","# Our hypothesis XW+b -> 선형 함수\n","# hypothesis = W * x_train # + b -> bias 일단 없애고 테스트. # 이 시점에서 10을 곱하는 게 아니고, 나중에 run할 때 곱해짐\n","hypothesis = W * x_train + b\n","\n","# hypothesis는 항상 W * x_train + b 여야 한다. 이런 식으로 바꾸면 안된다. hypothesis = x_train + x_train * W\n","# 나중에 뉴런 네트워크를 연결해서 값을 변경할 것\n","\n","# cost/loss function -> 제곱해서 평균내는 것\n","cost = tf.reduce_mean(tf.square(hypothesis - y_train)) # 이 때 10 곱함\n","\n","# optimizer\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(cost) # learning_rate가 클 수록 변하는 정도(?)가 커짐 - 학습효율 좋아짐. but 잘못하면 넘어가버리는 수가 있음\n","\n","# Launch the graph in a session.\n","sess = tf.Session() # 세션 open\n","\n","sess.run(tf.global_variables_initializer()) # 세션 초기화\n","\n","print(\"------------------------------------\")\n","print(\"%15s %15s %15s %15s\" %(\"STEP\",\"W\",\"cost\", \"bias\"))\n","\n","for step in range(1,1000):\n","  _, tw, tc, th, tb = sess.run([train, W, cost, hypothesis, b]) # python만의 문법 - return이 4개. _: 첫 번째 리턴은 안받겠다. 나머지는 변수임\n","  \n","  # 20번에 한번씩 출력\n","  if step%20 == 0:\n","\n","    print(\"%15.7f %15.7f %15.7f %15.7f\" %(step, tw, tc, tb))\n","\n","''' 하나하나 반복하기\n","test_w = sess.run(W) # 이 시점에서의 W 값을 리턴해줌\n","print(\"W : \", test_w)\n","\n","print(sess.run(hypothesis))\n","print(sess.run(cost))\n","\n","\n","print(\"----------1회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------2회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------3회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------4회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------5회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------6회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------7회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------8회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------9회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","\n","print(\"----------10회차----------\")\n","sess.run(train) # 핵심 코드. 얘가 있어야 W가 바뀜\n","print(sess.run(W)) \n","print(sess.run(cost)) \n","print(sess.run(hypothesis)) \n","'''\n","sess.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSl8HPmdTTff","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"ok","timestamp":1594945978627,"user_tz":-540,"elapsed":3464,"user":{"displayName":"최재현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH5rHXYDmngKeA0hKEsYsPu-A8L596LgtWX-h8=s64","userId":"17623597753724513855"}},"outputId":"3a24b2e4-f3ee-489e-dc74-061ba49259e7"},"source":["# 이제 머신러닝에게 질문을 해보자!\n","# 몇시에 데이터의 값은 무엇이냐??라고!\n","\n","\n","# Lab 2 Linear Regression - 선형 회귀\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility(재현성)-> 해도 되고 안해도 되는 코드\n","\n","# X and Y data -> training set\n","x_train = [1, 2, 3, 7, 7, 7, 7, 7, 7]\n","y_train = [1, 2, 3, 1, 4, 3, 2, 1, 7]\n","\n","W = tf.Variable(tf.random_normal([1])) # random엔 0값은 웬만하면 주지말라고 되어 있다.\n","b = tf.Variable(tf.random_normal([1]))\n","\n","\n","# Try to find values for W and b to compute y_data = x_data * W + b\n","# We know that W should be 1 and b should be 0\n","# But let TensorFlow figure it out\n","# weight는 10, bias는 0으로 시작하겠다. \n","W = tf.Variable(10.0) # W = 10으로 하지 않음. Variable클래스를 통해 자동으로 변경해야할 변수를 만듦\n","b = tf.Variable(10.0)\n","\n","# Our hypothesis XW+b -> 선형 함수\n","# hypothesis = W * x_train # + b -> bias 일단 없애고 테스트. # 이 시점에서 10을 곱하는 게 아니고, 나중에 run할 때 곱해짐\n","hypothesis = W * x_train + b\n","\n","# hypothesis는 항상 W * x_train + b 여야 한다. 이런 식으로 바꾸면 안된다. hypothesis = x_train + x_train * W\n","# 나중에 뉴런 네트워크를 연결해서 값을 변경할 것\n","\n","# cost/loss function -> 제곱해서 평균내는 것\n","cost = tf.reduce_mean(tf.square(hypothesis - y_train)) # 이 때 10 곱함\n","\n","# optimizer\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost) # learning_rate가 클 수록 변하는 정도(?)가 커짐 - 학습효율 좋아짐. but 잘못하면 넘어가버리는 수가 있음\n","\n","# Launch the graph in a session.\n","sess = tf.Session() # 세션 open\n","\n","sess.run(tf.global_variables_initializer()) # 세션 초기화\n","\n","print(\"------------------------------------\")\n","print(\"%15s %15s %15s\" %(\"STEP\",\"W\",\"cost\"))\n","\n","for step in range(1,1000):\n","  _, tw, tc, th = sess.run([train, W, cost, hypothesis]) # python만의 문법 - return이 4개. _: 첫 번째 리턴은 안받겠다. 나머지는 변수임\n","  \n","  # 20번에 한번씩 출력\n","  if step%2000 == 0:\n","    print(\"%15d %15.7f %15.7f\" %(step, tw, tc))\n","'''\n","q = 5\n","hypothesis = W * q + b\n","print(\"5의 결과는 무엇입니까?\")\n","print(sess.run(hypothesis))\n","'''\n","X = tf.placeholder(tf.float32, shape=[None])\n","Y = tf.placeholder(tf.float32, shape=[None])\n","\n","q = input(\"어떤 숫자를 예측하고 싶습니까?? > \")\n","q = int(q)\n","#hypothesis = W * q + b\n","\n","print(\"예측 결과 : \")\n","print(sess.run(hypothesis, feed_dict={X:[q]}))\n","\n","sess.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------------------------------------\n","           STEP               W            cost\n","어떤 숫자를 예측하고 싶습니까?? > 20\n","예측 결과 : \n","[5.8324766 5.2897606 4.7470446 2.57618   2.57618   2.57618   2.57618\n"," 2.57618   2.57618  ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2OBpG8uJMeda","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1594946095728,"user_tz":-540,"elapsed":3405,"user":{"displayName":"최재현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH5rHXYDmngKeA0hKEsYsPu-A8L596LgtWX-h8=s64","userId":"17623597753724513855"}},"outputId":"15eb655c-384a-46df-cbfb-5ea52233de1b"},"source":["#선형 회귀....\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","# X and Y data\n","x_train = [1,2,3,7,7,7,7,7,7] \n","y_train = [1,2,3,1,4,3,2,1,7]\n","\n","\n","W = tf.Variable(tf.random_normal([1]))\n","b = tf.Variable(tf.random_normal([1]))\n","\n","X = tf.placeholder(tf.float32, shape=[None])\n","Y = tf.placeholder(tf.float32, shape=[None])\n","\n","\n","\n","hypothesis = X * W + b\n","\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n","\n","\n","sess = tf.Session()\n","\n","sess.run(tf.global_variables_initializer())\n","\n","print(\"%15s %15s %15s\" %(\"STEP\",\"W\",\"COST\"))\n","\n","for step in range(1,1000):\n","    _,tw,tc,th,tb = sess.run([train,W,cost,hypothesis,b] , feed_dict={X:x_train , Y:y_train})\n","\n","    if step%2000 == 0:\n","        print(\"%15d %15.7f %15.7f %15.7f\" %(step,tw,tc,tb))\n","\n","\n","q = input(\"어떤 숫자를 예측하고 싶습니까??>\")\n","q = int(q)\n","#hypothesis = W * q + b\n","\n","\n","\n","print(\"예측 결과 : \")\n","print(sess.run(hypothesis , feed_dict={X:[q]}))\n","\n","\n","sess.close()\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["           STEP               W            COST\n","어떤 숫자를 예측하고 싶습니까??>20\n","예측 결과 : \n","[8.532329]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C1gLFxeoOTOq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"status":"ok","timestamp":1594946571361,"user_tz":-540,"elapsed":688,"user":{"displayName":"최재현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH5rHXYDmngKeA0hKEsYsPu-A8L596LgtWX-h8=s64","userId":"17623597753724513855"}},"outputId":"00f02d5c-9b4c-497c-8f97-563b154423f6"},"source":["# Lab 3 Minimizing Cost\n","import tensorflow as tf\n","\n","tf.set_random_seed(777)  # for reproducibility\n","\n","x_data = [1, 2, 3]\n","y_data = [1, 2, 3]\n","\n","# Try to find values for W and b to compute y_data = W * x_data\n","# We know that W should be 1\n","# But let's use TensorFlow to figure it out\n","W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n","\n","X = tf.placeholder(tf.float32)\n","Y = tf.placeholder(tf.float32)\n","\n","# Our hypothesis for linear model X * W\n","hypothesis = X * W\n","\n","# cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n","learning_rate = 0.1\n","gradient = tf.reduce_mean((W * X - Y) * X)\n","descent = W - learning_rate * gradient\n","update = W.assign(descent)\n","\n","# Launch the graph in a session.\n","with tf.Session() as sess:\n","    # Initializes global variables in the graph.\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(21):\n","        _, cost_val, W_val = sess.run(\n","            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n","        )\n","        print(step, cost_val, W_val)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 1.9812318 [0.6524935]\n","1 0.5635503 [0.8146632]\n","2 0.16029878 [0.9011537]\n","3 0.04559608 [0.94728196]\n","4 0.012969551 [0.9718837]\n","5 0.0036891224 [0.98500466]\n","6 0.0010493472 [0.9920025]\n","7 0.000298481 [0.99573463]\n","8 8.490288e-05 [0.9977251]\n","9 2.4149633e-05 [0.99878675]\n","10 6.869103e-06 [0.99935293]\n","11 1.9539195e-06 [0.9996549]\n","12 5.5572485e-07 [0.99981594]\n","13 1.5809626e-07 [0.99990183]\n","14 4.496154e-08 [0.99994767]\n","15 1.2768268e-08 [0.9999721]\n","16 3.6312713e-09 [0.9999851]\n","17 1.0397656e-09 [0.9999921]\n","18 2.9421798e-10 [0.99999577]\n","19 8.3073104e-11 [0.99999774]\n","20 2.3405278e-11 [0.9999988]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ef5aKvwOVWLw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594951509611,"user_tz":-540,"elapsed":29386,"user":{"displayName":"최재현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH5rHXYDmngKeA0hKEsYsPu-A8L596LgtWX-h8=s64","userId":"17623597753724513855"}},"outputId":"a5a19417-3588-44fc-d1e5-bdc854096cc7"},"source":["#https://github.com/emgjh92/DeepLearningZeroToAll/blob/master/lab-04-1-multi_variable_linear_regression.py\n","import tensorflow as tf\n","\n","tf.set_random_seed(777)  # for reproducibility\n","\n","x1_data = [1., 1., 3., 2., 1.]\n","x2_data = [1., 2., 4., 1., 3.]\n","x3_data = [1., 3., 5., 3., 2.]#X데이터\n","\n","y_data = [3., 5., 11., 7., 4.]#Y데이터\n","\n","x1 = tf.placeholder(tf.float32)\n","x2 = tf.placeholder(tf.float32)\n","x3 = tf.placeholder(tf.float32)# x placeholder\n","\n","Y = tf.placeholder(tf.float32)# y placeholder\n","\n","\n","\n","w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n","w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n","w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n","b = tf.Variable(tf.random_normal([1]), name='bias') #변수 선언\n","\n","\n","\n","hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n","\n","# cost/loss function\n","cost = tf.reduce_mean(tf.square(hypothesis - Y))\n","\n","# Minimize. Need a very small learning rate for this data set\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n","train = optimizer.minimize(cost)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","print(\"%10s %10s %10s %10s %10s %10s\" %(\"step\",\"w1\",\"w2\",\"w3\",\"b\",\"cost\"))\n","\n","for step in range(1,50000):\n","    sess.run(train , feed_dict= {x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n","\n","    if step%500 == 1:\n","        tw1,tw2,tw3,tb,tc = sess.run([w1,w2,w3,b,cost] , feed_dict= {x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n","        print(\"%10d %10.7f %10.7f %10.7f %10.7f %10.7f\" %(step,tw1,tw2,tw3,tb,tc))\n","\n","\n","sess.close()\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["      step         w1         w2         w3          b       cost\n","         1  1.4959168  0.4392838  0.9234905 -1.2120464  1.8275677\n","       501  1.7361668  0.3520834  1.1674856 -0.9816306  0.2403528\n","      1001  1.8167137  0.2189043  1.1819205 -0.8337866  0.1463973\n","      1501  1.8669454  0.1420003  1.1761189 -0.7082200  0.0974651\n","      2001  1.8996103  0.0965452  1.1612988 -0.6015728  0.0678443\n","      2501  1.9218472  0.0688318  1.1432984 -0.5109932  0.0481893\n","      3001  1.9376916  0.0512650  1.1250570 -0.4340603  0.0345292\n","      3501  1.9494636  0.0396198  1.1079521 -0.3687167  0.0248333\n","      4001  1.9585214  0.0315227  1.0925524 -0.3132159  0.0178877\n","      4501  1.9656847  0.0256274  1.0790031 -0.2660751  0.0128934\n","      5001  1.9714649  0.0211568  1.0672462 -0.2260339  0.0092960\n","      5501  1.9761962  0.0176518  1.0571318 -0.1920228  0.0067031\n","      6001  1.9801074  0.0148338  1.0484769 -0.1631333  0.0048337\n","      6501  1.9833605  0.0125257  1.0410973 -0.1385935  0.0034858\n","      7001  1.9860778  0.0106109  1.0348191 -0.1177484  0.0025138\n","      7501  1.9883522  0.0090083  1.0294858 -0.1000414  0.0018128\n","      8001  1.9902602  0.0076587  1.0249602 -0.0849999  0.0013074\n","      8501  1.9918615  0.0065179  1.0211228 -0.0722222  0.0009429\n","      9001  1.9932042  0.0055509  1.0178709 -0.0613674  0.0006800\n","      9501  1.9943297  0.0047296  1.0151153 -0.0521460  0.0004905\n","     10001  1.9952762  0.0040313  1.0127828 -0.0443121  0.0003537\n","     10501  1.9960663  0.0034374  1.0108069 -0.0376565  0.0002552\n","     11001  1.9967306  0.0029309  1.0091350 -0.0320022  0.0001840\n","     11501  1.9972875  0.0024998  1.0077207 -0.0271983  0.0001328\n","     12001  1.9977506  0.0021339  1.0065218 -0.0231161  0.0000958\n","     12501  1.9981382  0.0018209  1.0055088 -0.0196479  0.0000691\n","     13001  1.9984645  0.0015545  1.0046525 -0.0167010  0.0000498\n","     13501  1.9987346  0.0013276  1.0039276 -0.0141968  0.0000360\n","     14001  1.9989702  0.0011317  1.0033097 -0.0120697  0.0000259\n","     14501  1.9991490  0.0009674  1.0027974 -0.0102610  0.0000187\n","     15001  1.9993097  0.0008275  1.0023535 -0.0087240  0.0000135\n","     15501  1.9994309  0.0007083  1.0019865 -0.0074172  0.0000098\n","     16001  1.9995500  0.0006008  1.0016743 -0.0063089  0.0000070\n","     16501  1.9996349  0.0005146  1.0014099 -0.0053656  0.0000051\n","     17001  1.9996945  0.0004466  1.0011886 -0.0045603  0.0000037\n","     17501  1.9997541  0.0003758  1.0010070 -0.0038806  0.0000027\n","     18001  1.9998137  0.0003257  1.0008383 -0.0032990  0.0000019\n","     18501  1.9998629  0.0002735  1.0007054 -0.0028089  0.0000014\n","     19001  1.9999024  0.0002354  1.0005862 -0.0023902  0.0000010\n","     19501  1.9999216  0.0002063  1.0004915 -0.0020324  0.0000007\n","     20001  1.9999216  0.0001728  1.0004280 -0.0017307  0.0000005\n","     20501  1.9999216  0.0001482  1.0003684 -0.0014723  0.0000004\n","     21001  1.9999267  0.0001340  1.0003088 -0.0012486  0.0000003\n","     21501  1.9999329  0.0001201  1.0002594 -0.0010578  0.0000002\n","     22001  1.9999329  0.0001026  1.0002255 -0.0008972  0.0000001\n","     22501  1.9999329  0.0000849  1.0001994 -0.0007618  0.0000001\n","     23001  1.9999329  0.0000685  1.0001783 -0.0006474  0.0000001\n","     23501  1.9999329  0.0000540  1.0001611 -0.0005508  0.0000001\n","     24001  1.9999329  0.0000415  1.0001465 -0.0004691  0.0000000\n","     24501  1.9999329  0.0000308  1.0001345 -0.0004001  0.0000000\n","     25001  1.9999329  0.0000216  1.0001242 -0.0003417  0.0000000\n","     25501  1.9999329  0.0000139  1.0001156 -0.0002923  0.0000000\n","     26001  1.9999329  0.0000073  1.0001084 -0.0002505  0.0000000\n","     26501  1.9999329  0.0000018  1.0001023 -0.0002151  0.0000000\n","     27001  1.9999329 -0.0000029  1.0000969 -0.0001852  0.0000000\n","     27501  1.9999329 -0.0000068  1.0000926 -0.0001600  0.0000000\n","     28001  1.9999329 -0.0000102  1.0000889 -0.0001385  0.0000000\n","     28501  1.9999329 -0.0000130  1.0000856 -0.0001205  0.0000000\n","     29001  1.9999329 -0.0000154  1.0000830 -0.0001051  0.0000000\n","     29501  1.9999329 -0.0000174  1.0000807 -0.0000922  0.0000000\n","     30001  1.9999329 -0.0000192  1.0000788 -0.0000812  0.0000000\n","     30501  1.9999329 -0.0000207  1.0000772 -0.0000720  0.0000000\n","     31001  1.9999329 -0.0000219  1.0000759 -0.0000642  0.0000000\n","     31501  1.9999329 -0.0000229  1.0000747 -0.0000576  0.0000000\n","     32001  1.9999329 -0.0000238  1.0000738 -0.0000520  0.0000000\n","     32501  1.9999329 -0.0000245  1.0000730 -0.0000472  0.0000000\n","     33001  1.9999329 -0.0000252  1.0000722 -0.0000432  0.0000000\n","     33501  1.9999329 -0.0000257  1.0000716 -0.0000398  0.0000000\n","     34001  1.9999329 -0.0000263  1.0000713 -0.0000370  0.0000000\n","     34501  1.9999329 -0.0000267  1.0000709 -0.0000346  0.0000000\n","     35001  1.9999329 -0.0000270  1.0000705 -0.0000326  0.0000000\n","     35501  1.9999329 -0.0000272  1.0000701 -0.0000308  0.0000000\n","     36001  1.9999329 -0.0000275  1.0000699 -0.0000293  0.0000000\n","     36501  1.9999329 -0.0000276  1.0000697 -0.0000281  0.0000000\n","     37001  1.9999329 -0.0000279  1.0000696 -0.0000271  0.0000000\n","     37501  1.9999329 -0.0000280  1.0000694 -0.0000262  0.0000000\n","     38001  1.9999329 -0.0000280  1.0000693 -0.0000254  0.0000000\n","     38501  1.9999329 -0.0000282  1.0000691 -0.0000248  0.0000000\n","     39001  1.9999329 -0.0000283  1.0000691 -0.0000243  0.0000000\n","     39501  1.9999329 -0.0000284  1.0000690 -0.0000238  0.0000000\n","     40001  1.9999329 -0.0000286  1.0000690 -0.0000234  0.0000000\n","     40501  1.9999329 -0.0000285  1.0000689 -0.0000231  0.0000000\n","     41001  1.9999329 -0.0000285  1.0000689 -0.0000229  0.0000000\n","     41501  1.9999329 -0.0000286  1.0000689 -0.0000227  0.0000000\n","     42001  1.9999329 -0.0000287  1.0000689 -0.0000226  0.0000000\n","     42501  1.9999329 -0.0000287  1.0000689 -0.0000225  0.0000000\n","     43001  1.9999329 -0.0000287  1.0000688 -0.0000223  0.0000000\n","     43501  1.9999329 -0.0000288  1.0000688 -0.0000222  0.0000000\n","     44001  1.9999329 -0.0000288  1.0000688 -0.0000221  0.0000000\n","     44501  1.9999329 -0.0000288  1.0000688 -0.0000220  0.0000000\n","     45001  1.9999329 -0.0000288  1.0000688 -0.0000220  0.0000000\n","     45501  1.9999329 -0.0000288  1.0000688 -0.0000220  0.0000000\n","     46001  1.9999329 -0.0000288  1.0000688 -0.0000220  0.0000000\n","     46501  1.9999329 -0.0000288  1.0000688 -0.0000219  0.0000000\n","     47001  1.9999329 -0.0000288  1.0000688 -0.0000219  0.0000000\n","     47501  1.9999329 -0.0000288  1.0000688 -0.0000219  0.0000000\n","     48001  1.9999329 -0.0000288  1.0000688 -0.0000218  0.0000000\n","     48501  1.9999329 -0.0000287  1.0000687 -0.0000217  0.0000000\n","     49001  1.9999329 -0.0000286  1.0000685 -0.0000217  0.0000000\n","     49501  1.9999329 -0.0000286  1.0000685 -0.0000216  0.0000000\n"],"name":"stdout"}]}]}