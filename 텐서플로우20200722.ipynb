{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"텐서플로우20200722.ipynb","provenance":[],"authorship_tag":"ABX9TyMvr6n4wf2l+XzpV5OmB+Mb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RfYLkT8k1rtq","colab_type":"code","colab":{}},"source":["!pip install tensorflow==1.15.2\n","#설치후 런타임 -> 런타임 다시 실행 해 줘야 함"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTulpexu66Fc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1595381571820,"user_tz":-540,"elapsed":966,"user":{"displayName":"최재현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjH5rHXYDmngKeA0hKEsYsPu-A8L596LgtWX-h8=s64","userId":"17623597753724513855"}},"outputId":"1454e51e-5987-426b-a7b4-854192a55e7f"},"source":["#Multinomal classification\n","#http://hunkim.github.io/ml/lec6.pdf\n","#https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-06-1-softmax_classifier.py\n","#softmax_classifier\n","\n","\n","# Lab 6 Softmax Classifier\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","\n","x_data = [[1, 2, 1, 1],\n","          [2, 1, 3, 2],\n","          [3, 1, 3, 4],\n","          [4, 1, 5, 5],\n","          [1, 7, 5, 5],\n","          [1, 2, 5, 6],\n","          [1, 6, 6, 6],\n","          [1, 7, 7, 7]]\n","y_data = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]]\n","#y_date 의 값이 가지는 경우의 수가 딱 세가지 인 경우 하나만 1 이 들어가게 된다.\n","\n","X = tf.placeholder(\"float\", [None, 4]) #행의갯수 상관없고 입력 네개\n","Y = tf.placeholder(\"float\", [None, 3]) #행의갯수 상관 없고 입력 세개\n","\n","nb_classes = 3\n","\n","W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n","b = tf.Variable(tf.random_normal([nb_classes]), name='bias') #bias 는 출력 갯수만큼 만들어 주면 된다.\n","\n","# tf.nn.softmax computes softmax activations\n","# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n","hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n","\n","# Cross entropy cost/loss\n","cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n","\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) #train\n","\n","# Launch graph\n","with tf.Session() as sess:\n","\n","    sess.run(tf.global_variables_initializer()) #변수 초기화\n","\n","    for step in range(50000):\n","      sess.run(train,feed_dict={X: x_data, Y: y_data}) #학습 코드\n","\n","      if step%1000 == 0:\n","        tc = sess.run(cost,feed_dict={X: x_data, Y: y_data})\n","\n","        print(\"%10d %10.7f\" %(step,tc))\n","    \n","    x_test = [\n","              [1, 7, 5, 5]\n","              ]\n","    result = sess.run(hypothesis, feed_dict={X: x_test, Y: y_data})\n","\n","###############result 를 1 혹은 0 만 나오도록 결과값 수정################\n","    prediction = tf.argmax(hypothesis,1)\n","    tp = sess.run(prediction,feed_dict={X: x_test})\n","########################################################################\n","    print(\"결과 :\",tp)\n","\n","\n","\n","\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["         0  5.9056001\n","결과 : [0]\n"],"name":"stdout"}]}]}